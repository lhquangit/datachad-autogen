{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae1f50ec",
   "metadata": {
    "id": "ae1f50ec"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/microsoft/autogen/blob/main/notebook/agentchat_langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a71fa36",
   "metadata": {
    "id": "9a71fa36"
   },
   "source": [
    "# Auto Generated Agent Chat: Task Solving with Langchain Provided Tools as Functions\n",
    "\n",
    "AutoGen offers conversable agents powered by LLM, tool, or human, which can be used to perform tasks collectively via automated chat. This framework allows tool use and human participants through multi-agent conversation. Please find documentation about this feature [here](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat).\n",
    "\n",
    "In this notebook, we demonstrate how to use `AssistantAgent` and `UserProxyAgent` to make function calls with the new feature of OpenAI models (in model version 0613) with a set of Langchain-provided tools and toolkits, to demonstrate how to leverage the 35+ tools available. \n",
    "A specified prompt and function configs must be passed to `AssistantAgent` to initialize the agent. The corresponding functions must be passed to `UserProxyAgent`, which will execute any function calls made by `AssistantAgent`. Besides this requirement of matching descriptions with functions, we recommend checking the system message in the `AssistantAgent` to ensure the instructions align with the function call descriptions.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "AutoGen requires `Python>=3.8`. To run this notebook example, please install `pyautogen` and `Langchain`:\n",
    "```bash\n",
    "pip install pyautogen Langchain\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b803c17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2b803c17",
    "outputId": "2e12aa3f-e46c-4b82-cc2e-1495f70a2961"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyautogen>=0.2.3 in /home/quan/anaconda3/envs/autogen/lib/python3.10/site-packages (0.2.15)\n",
      "Requirement already satisfied: Langchain in /home/quan/anaconda3/envs/autogen/lib/python3.10/site-packages (0.1.9)\n",
      "Requirement already satisfied: diskcache in /home/quan/anaconda3/envs/autogen/lib/python3.10/site-packages (from pyautogen>=0.2.3) (5.6.3)\n",
      "Requirement already satisfied: docker in /home/quan/anaconda3/envs/autogen/lib/python3.10/site-packages (from pyautogen>=0.2.3) (7.0.0)\n",
      "Requirement already satisfied: flaml in /home/quan/anaconda3/envs/autogen/lib/python3.10/site-packages (from pyautogen>=0.2.3) (2.1.1)\n",
      "Requirement already satisfied: openai>=1.3 in /home/quan/anaconda3/envs/autogen/lib/python3.10/site-packages (from pyautogen>=0.2.3) (1.12.0)\n",
      "Requirement already satisfied: pydantic!=2.6.0,<3,>=1.10 in /home/quan/anaconda3/envs/autogen/lib/python3.10/site-packages (from pyautogen>=0.2.3) (2.6.2)\n",
      "Requirement already satisfied: python-dotenv in /home/quan/anaconda3/envs/autogen/lib/python3.10/site-packages (from pyautogen>=0.2.3) (1.0.1)\n",
      "Requirement already satisfied: termcolor in /home/quan/anaconda3/envs/autogen/lib/python3.10/site-packages (from pyautogen>=0.2.3) (2.4.0)\n",
      "Requirement already satisfied: tiktoken in /home/quan/anaconda3/envs/autogen/lib/python3.10/site-packages (from pyautogen>=0.2.3) (0.6.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/quan/anaconda3/envs/autogen/lib/python3.10/site-packages (from Langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/quan/anaconda3/envs/autogen/lib/python3.10/site-packages (from Langchain) (2.0.27)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/quan/anaconda3/envs/autogen/lib/python3.10/site-packages (from Langchain) (3.9.3)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/quan/anaconda3/envs/autogen/lib/python3.10/site-packages (from Langchain) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/quan/anaconda3/envs/autogen/lib/python3.10/site-packages (from Langchain) (0.6.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/quan/anaconda3/envs/autogen/lib/python3.10/site-packages (from Langchain) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.21 in /home/quan/anaconda3/envs/autogen/lib/python3.10/site-packages (from Langchain) (0.0.24)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.26 in /home/quan/anaconda3/envs/autogen/lib/python3.10/site-packages (from Langchain) (0.1.27)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /home/quan/anaconda3/envs/autogen/lib/python3.10/site-packages (from Langchain) (0.1.9)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/quan/anaconda3/envs/autogen/lib/python3.10/site-packages (from Langchain) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/quan/anaconda3/envs/autogen/lib/python3.10/site-packages (from Langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/quan/anaconda3/envs/autogen/lib/python3.10/site-packages (from Langchain) (8.2.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/quan/anaconda3/envs/autogen/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->Langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/quan/anaconda3/envs/autogen/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->Langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/quan/anaconda3/envs/autogen/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->Langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/quan/anaconda3/envs/autogen/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->Langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/quan/anaconda3/envs/autogen/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->Langchain) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/quan/anaconda3/envs/autogen/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->Langchain) (3.21.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/quan/anaconda3/envs/autogen/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->Langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/quan/anaconda3/envs/autogen/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->Langchain) (2.4)\n",
      "Requirement already satisfied: anyio<5,>=3 in /home/quan/anaconda3/envs/autogen/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.26->Langchain) (4.3.0)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /home/quan/anaconda3/envs/autogen/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.26->Langchain) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/quan/anaconda3/envs/autogen/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.0->Langchain) (3.9.15)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/quan/anaconda3/envs/autogen/lib/python3.10/site-packages (from openai>=1.3->pyautogen>=0.2.3) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/quan/anaconda3/envs/autogen/lib/python3.10/site-packages (from openai>=1.3->pyautogen>=0.2.3) (0.27.0)\n",
      "Requirement already satisfied: sniffio in /home/quan/anaconda3/envs/autogen/lib/python3.10/site-packages (from openai>=1.3->pyautogen>=0.2.3) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /home/quan/anaconda3/envs/autogen/lib/python3.10/site-packages (from openai>=1.3->pyautogen>=0.2.3) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/quan/anaconda3/envs/autogen/lib/python3.10/site-packages (from openai>=1.3->pyautogen>=0.2.3) (4.10.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/quan/anaconda3/envs/autogen/lib/python3.10/site-packages (from pydantic!=2.6.0,<3,>=1.10->pyautogen>=0.2.3) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /home/quan/anaconda3/envs/autogen/lib/python3.10/site-packages (from pydantic!=2.6.0,<3,>=1.10->pyautogen>=0.2.3) (2.16.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/quan/anaconda3/envs/autogen/lib/python3.10/site-packages (from requests<3,>=2->Langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/quan/anaconda3/envs/autogen/lib/python3.10/site-packages (from requests<3,>=2->Langchain) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/quan/anaconda3/envs/autogen/lib/python3.10/site-packages (from requests<3,>=2->Langchain) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/quan/anaconda3/envs/autogen/lib/python3.10/site-packages (from requests<3,>=2->Langchain) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/quan/anaconda3/envs/autogen/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->Langchain) (3.0.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/quan/anaconda3/envs/autogen/lib/python3.10/site-packages (from tiktoken->pyautogen>=0.2.3) (2023.12.25)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/quan/anaconda3/envs/autogen/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.26->Langchain) (1.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/quan/anaconda3/envs/autogen/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai>=1.3->pyautogen>=0.2.3) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/quan/anaconda3/envs/autogen/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.3->pyautogen>=0.2.3) (0.14.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/quan/anaconda3/envs/autogen/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->Langchain) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install \"pyautogen>=0.2.3\" Langchain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5ebd2397",
   "metadata": {
    "id": "5ebd2397"
   },
   "source": [
    "## Set your API Endpoint\n",
    "\n",
    "The [`config_list_from_models`](https://microsoft.github.io/autogen/docs/reference/oai/openai_utils#config_list_from_models) function tries to create a list of configurations using Azure OpenAI endpoints and OpenAI endpoints for the provided list of models. It assumes the api keys and api bases are stored in the corresponding environment variables or local txt files:\n",
    "\n",
    "- OpenAI API key: os.environ[\"OPENAI_API_KEY\"] or `openai_api_key_file=\"key_openai.txt\"`.\n",
    "- Azure OpenAI API key: os.environ[\"AZURE_OPENAI_API_KEY\"] or `aoai_api_key_file=\"key_aoai.txt\"`. Multiple keys can be stored, one per line.\n",
    "- Azure OpenAI API base: os.environ[\"AZURE_OPENAI_API_BASE\"] or `aoai_api_base_file=\"base_aoai.txt\"`. Multiple bases can be stored, one per line.\n",
    "\n",
    "It's OK to have only the OpenAI API key, or only the Azure OpenAI API key + base.\n",
    "If you open this notebook in google colab, you can upload your files by clicking the file icon on the left panel and then choosing \"upload file\" icon.\n",
    "\n",
    "The following code excludes Azure OpenAI endpoints from the config list because some endpoints don't support functions yet. Remove the `exclude` argument if they do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dca301a4",
   "metadata": {
    "id": "dca301a4"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from typing import Optional, Type\n",
    "\n",
    "# Starndard Langchain example\n",
    "from langchain.agents import create_spark_sql_agent\n",
    "from langchain.agents.agent_toolkits import SparkSQLToolkit\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Import things that are needed generically\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain.tools import BaseTool\n",
    "from langchain.tools.file_management.read import ReadFileTool\n",
    "from langchain.utilities.spark_sql import SparkSQL\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import autogen\n",
    "\n",
    "config_list = autogen.config_list_from_json(\n",
    "    # \"OAI_CONFIG_LIST\",\n",
    "    \"config.json\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-4\", \"gpt-3.5-turbo\", \"gpt-3.5-turbo-16k\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd1cda81",
   "metadata": {},
   "source": [
    "It first looks for environment variable \"OAI_CONFIG_LIST\" which needs to be a valid json string. If that variable is not found, it then looks for a json file named \"OAI_CONFIG_LIST\". It filters the configs by models (you can filter by other keys as well). Only the models with matching names are kept in the list based on the filter condition.\n",
    "\n",
    "The config list looks like the following:\n",
    "```python\n",
    "config_list = [\n",
    "    {\n",
    "        'model': 'gpt-4',\n",
    "        'api_key': '<your OpenAI API key here>',\n",
    "    },\n",
    "    {\n",
    "        'model': 'gpt-3.5-turbo',\n",
    "        'api_key': '<your Azure OpenAI API key here>',\n",
    "        'base_url': '<your Azure OpenAI API base here>',\n",
    "        'api_type': 'azure',\n",
    "        'api_version': '2024-02-15-preview',\n",
    "    },\n",
    "    {\n",
    "        'model': 'gpt-3.5-turbo-16k',\n",
    "        'api_key': '<your Azure OpenAI API key here>',\n",
    "        'base_url': '<your Azure OpenAI API base here>',\n",
    "        'api_type': 'azure',\n",
    "        'api_version': '2024-02-15-preview',\n",
    "    },\n",
    "]\n",
    "```\n",
    "\n",
    "You can set the value of config_list in any way you prefer. Please refer to this [notebook](https://github.com/microsoft/autogen/blob/main/website/docs/llm_configuration.ipynb) for full code examples of the different methods."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b9526e7",
   "metadata": {
    "id": "2b9526e7"
   },
   "source": [
    "## Making Function Calls\n",
    "\n",
    "In this example, we demonstrate function call execution with `AssistantAgent` and `UserProxyAgent`. With the default system prompt of `AssistantAgent`, we allow the LLM assistant to perform tasks with code, and the `UserProxyAgent` would extract code blocks from the LLM response and execute them. With the new \"function_call\" feature, we define functions and specify the description of the function in the OpenAI config for the `AssistantAgent`. Then we register the functions in `UserProxyAgent`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "qCzNbbVajvpc",
   "metadata": {
    "id": "qCzNbbVajvpc"
   },
   "outputs": [],
   "source": [
    "class CircumferenceToolInput(BaseModel):\n",
    "    radius: float = Field()\n",
    "\n",
    "\n",
    "class CircumferenceTool(BaseTool):\n",
    "    name = \"circumference_calculator\"\n",
    "    description = \"Use this tool when you need to calculate a circumference using the radius of a circle\"\n",
    "    args_schema: Type[BaseModel] = CircumferenceToolInput\n",
    "\n",
    "    def _run(self, radius: float):\n",
    "        return float(radius) * 2.0 * math.pi\n",
    "\n",
    "\n",
    "def get_file_path_of_example():\n",
    "    # Get the current working directory\n",
    "    current_dir = os.getcwd()\n",
    "\n",
    "    # Go one directory up\n",
    "    # parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "    # Move to the target directory\n",
    "    target_folder = os.path.join(current_dir, \"test\")\n",
    "\n",
    "    # Construct the path to your target file\n",
    "    file_path = os.path.join(target_folder, \"test_files/radius.txt\")\n",
    "    print(file_path)\n",
    "\n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "COlL5_98atDs",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "COlL5_98atDs",
    "outputId": "24ce236d-8993-4a69-99e2-65453574d61e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/quan/Projects/30_DataChad/test/test_files/radius.txt\n",
      "\u001b[33muser_proxy\u001b[0m (to chatbot):\n",
      "\n",
      "Read the file with the path /home/quan/Projects/30_DataChad/test/test_files/radius.txt, then calculate the circumference of a circle that has a radius of that files contents.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mchatbot\u001b[0m (to user_proxy):\n",
      "\n",
      "\u001b[32m***** Suggested function Call: read_file *****\u001b[0m\n",
      "Arguments: \n",
      "{\"file_path\":\"/home/quan/Projects/30_DataChad/test/test_files/radius.txt\"}\n",
      "\u001b[32m**********************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION read_file...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to chatbot):\n",
      "\n",
      "\u001b[32m***** Response from calling function \"read_file\" *****\u001b[0m\n",
      "7.81mm\n",
      "\u001b[32m******************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mchatbot\u001b[0m (to user_proxy):\n",
      "\n",
      "\u001b[32m***** Suggested function Call: circumference_calculator *****\u001b[0m\n",
      "Arguments: \n",
      "{\"radius\":7.81}\n",
      "\u001b[32m*************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION circumference_calculator...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to chatbot):\n",
      "\n",
      "\u001b[32m***** Response from calling function \"circumference_calculator\" *****\u001b[0m\n",
      "49.071677249072565\n",
      "\u001b[32m*********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mchatbot\u001b[0m (to user_proxy):\n",
      "\n",
      "The circumference of a circle with a radius of 7.81mm is approximately 49.07.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muser_proxy\u001b[0m (to chatbot):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mchatbot\u001b[0m (to user_proxy):\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatResult(chat_id=None, chat_history=[{'content': 'Read the file with the path /home/quan/Projects/30_DataChad/test/test_files/radius.txt, then calculate the circumference of a circle that has a radius of that files contents.', 'role': 'assistant'}, {'function_call': {'arguments': '{\"file_path\":\"/home/quan/Projects/30_DataChad/test/test_files/radius.txt\"}', 'name': 'read_file'}, 'content': None, 'role': 'assistant'}, {'content': '7.81mm', 'name': 'read_file', 'role': 'function'}, {'function_call': {'arguments': '{\"radius\":7.81}', 'name': 'circumference_calculator'}, 'content': None, 'role': 'assistant'}, {'content': '49.071677249072565', 'name': 'circumference_calculator', 'role': 'function'}, {'content': 'The circumference of a circle with a radius of 7.81mm is approximately 49.07.', 'role': 'user'}, {'content': '', 'role': 'assistant'}, {'content': 'TERMINATE', 'role': 'user'}], summary='', cost=({'total_cost': 0, 'gpt-3.5-turbo-0125': {'cost': 0, 'prompt_tokens': 851, 'completion_tokens': 76, 'total_tokens': 927}}, {'total_cost': 0, 'gpt-3.5-turbo-0125': {'cost': 0, 'prompt_tokens': 696, 'completion_tokens': 46, 'total_tokens': 742}}), human_input=[])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to generate llm_config from a LangChain tool\n",
    "\n",
    "\n",
    "def generate_llm_config(tool):\n",
    "    # Define the function schema based on the tool's args_schema\n",
    "    function_schema = {\n",
    "        \"name\": tool.name.lower().replace(\" \", \"_\"),\n",
    "        \"description\": tool.description,\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {},\n",
    "            \"required\": [],\n",
    "        },\n",
    "    }\n",
    "\n",
    "    if tool.args is not None:\n",
    "        function_schema[\"parameters\"][\"properties\"] = tool.args\n",
    "\n",
    "    return function_schema\n",
    "\n",
    "\n",
    "# Instantiate the ReadFileTool\n",
    "read_file_tool = ReadFileTool()\n",
    "custom_tool = CircumferenceTool()\n",
    "\n",
    "# Construct the llm_config\n",
    "llm_config = {\n",
    "    # Generate functions config for the Tool\n",
    "    \"functions\": [\n",
    "        generate_llm_config(custom_tool),\n",
    "        generate_llm_config(read_file_tool),\n",
    "    ],\n",
    "    \"config_list\": config_list,  # Assuming you have this defined elsewhere\n",
    "    \"timeout\": 120,\n",
    "}\n",
    "\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\") and x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=10,\n",
    "    code_execution_config={\n",
    "        \"work_dir\": \"coding\",\n",
    "        \"use_docker\": False,\n",
    "    },  # Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.\n",
    ")\n",
    "\n",
    "# Register the tool and start the conversation\n",
    "user_proxy.register_function(\n",
    "    function_map={\n",
    "        custom_tool.name: custom_tool._run,\n",
    "        read_file_tool.name: read_file_tool._run,\n",
    "    }\n",
    ")\n",
    "\n",
    "chatbot = autogen.AssistantAgent(\n",
    "    name=\"chatbot\",\n",
    "    system_message=\"For coding tasks, only use the functions you have been provided with. Reply TERMINATE when the task is done.\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "user_proxy.initiate_chat(\n",
    "    chatbot,\n",
    "    message=f\"Read the file with the path {get_file_path_of_example()}, then calculate the circumference of a circle that has a radius of that files contents.\",  # 7.81mm in the file\n",
    "    llm_config=llm_config,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "11cc4e60",
   "metadata": {},
   "source": [
    "# A PySpark Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "Y-ozf9EFCegw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y-ozf9EFCegw",
    "outputId": "db7b73a8-6129-4dfb-9d5c-ac3536f310d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /home/quan/anaconda3/envs/autogen/lib/python3.10/site-packages (3.5.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /home/quan/anaconda3/envs/autogen/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7iFp-Sm4CYq_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7iFp-Sm4CYq_",
    "outputId": "2e1a2a70-53e6-4896-9232-63db6d097d63"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/27 14:00:08 WARN Utils: Your hostname, quan-desktop resolves to a loopback address: 127.0.1.1; using 10.20.1.62 instead (on interface enp0s31f6)\n",
      "24/02/27 14:00:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/27 14:00:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/home/quan/Projects/30_DataChad/sample_data/california_housing_train.csv.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m csv_file_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./sample_data/california_housing_train.csv\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m table \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcalifornia_housing_train\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m spark\u001b[39m.\u001b[39;49mread\u001b[39m.\u001b[39;49mcsv(csv_file_path, header\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, inferSchema\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\u001b[39m.\u001b[39mwrite\u001b[39m.\u001b[39moption(\n\u001b[1;32m      8\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mpath\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfile:/content/spark-warehouse/langchain_example.db/california_housing_train\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m )\u001b[39m.\u001b[39mmode(\u001b[39m\"\u001b[39m\u001b[39moverwrite\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39msaveAsTable(table)\n\u001b[1;32m     10\u001b[0m spark\u001b[39m.\u001b[39mtable(table)\u001b[39m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/anaconda3/envs/autogen/lib/python3.10/site-packages/pyspark/sql/readwriter.py:740\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(path) \u001b[39m==\u001b[39m \u001b[39mlist\u001b[39m:\n\u001b[1;32m    739\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_spark\u001b[39m.\u001b[39m_sc\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 740\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_df(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jreader\u001b[39m.\u001b[39;49mcsv(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_spark\u001b[39m.\u001b[39;49m_sc\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mPythonUtils\u001b[39m.\u001b[39;49mtoSeq(path)))\n\u001b[1;32m    741\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    743\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mfunc\u001b[39m(iterator):\n",
      "File \u001b[0;32m~/anaconda3/envs/autogen/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/autogen/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/home/quan/Projects/30_DataChad/sample_data/california_housing_train.csv."
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "schema = \"langchain_example\"\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {schema}\")\n",
    "spark.sql(f\"USE {schema}\")\n",
    "csv_file_path = \"./sample_data/california_housing_train.csv\"\n",
    "table = \"california_housing_train\"\n",
    "spark.read.csv(csv_file_path, header=True, inferSchema=True).write.option(\n",
    "    \"path\", \"file:/content/spark-warehouse/langchain_example.db/california_housing_train\"\n",
    ").mode(\"overwrite\").saveAsTable(table)\n",
    "spark.table(table).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iLtSTHoJD7Jn",
   "metadata": {
    "id": "iLtSTHoJD7Jn"
   },
   "outputs": [],
   "source": [
    "# Note, you can also connect to Spark via Spark connect. For example:\n",
    "# db = SparkSQL.from_uri(\"sc://localhost:15002\", schema=schema)\n",
    "spark_sql = SparkSQL(schema=schema)\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-16k\")\n",
    "toolkit = SparkSQLToolkit(db=spark_sql, llm=llm)\n",
    "agent_executor = create_spark_sql_agent(llm=llm, toolkit=toolkit, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VzqNYlVjCqQa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 608
    },
    "id": "VzqNYlVjCqQa",
    "outputId": "dd4de772-7b0c-4650-d106-c83d4593158e"
   },
   "outputs": [],
   "source": [
    "# Starndard Langchain example\n",
    "agent_executor.run(\"Describe the california_housing_train table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94d45a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain direct tool usage instead of toolkit example\n",
    "# from langchain.tools.spark_sql.tool import (\n",
    "#     InfoSparkSQLTool,\n",
    "#     ListSparkSQLTool,\n",
    "#     QueryCheckerTool,\n",
    "#     QuerySparkSQLTool,\n",
    "# )\n",
    "# debug_toolkit = [\n",
    "#   QuerySparkSQLTool(db=spark_sql),\n",
    "#   InfoSparkSQLTool(db=spark_sql),\n",
    "#   ListSparkSQLTool(db=spark_sql),\n",
    "#   QueryCheckerTool(db=spark_sql, llm=llm),\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r7PFvDS7Ev-E",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r7PFvDS7Ev-E",
    "outputId": "53d9c45d-058e-4e37-ba73-556591aaab42"
   },
   "outputs": [],
   "source": [
    "# Now use AutoGen with Langchain Tool Bridgre\n",
    "tools = []\n",
    "function_map = {}\n",
    "\n",
    "for tool in toolkit.get_tools():  # debug_toolkit if you want to use tools directly\n",
    "    tool_schema = generate_llm_config(tool)\n",
    "    print(tool_schema)\n",
    "    tools.append(tool_schema)\n",
    "    function_map[tool.name] = tool._run\n",
    "\n",
    "# Construct the llm_config\n",
    "llm_config = {\n",
    "    \"functions\": tools,\n",
    "    \"config_list\": config_list,  # Assuming you have this defined elsewhere\n",
    "    \"timeout\": 120,\n",
    "}\n",
    "\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\") and x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=10,\n",
    "    code_execution_config={\n",
    "        \"work_dir\": \"coding\",\n",
    "        \"use_docker\": False,\n",
    "    },  # Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.\n",
    ")\n",
    "\n",
    "print(function_map)\n",
    "\n",
    "# Register the tool and start the conversation\n",
    "user_proxy.register_function(function_map=function_map)\n",
    "\n",
    "chatbot = autogen.AssistantAgent(\n",
    "    name=\"chatbot\",\n",
    "    system_message=\"For coding tasks, only use the functions you have been provided with. Reply TERMINATE when the task is done.\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "user_proxy.initiate_chat(\n",
    "    chatbot,\n",
    "    message=\"Describe the table names california_housing_train\",\n",
    "    llm_config=llm_config,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "flaml_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
